{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import uuid\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "import boto3\n",
    "from helpers.utils import retrieve, get_max_batch_N, write_parquet\n",
    "from helpers.config import MAX_WORKERS, BULK_DATA_BUCKET_OR_PATH, FILE_KEY, BUCKET_OR_PATH_TO_SAVE, LOCAL_TMP_DIR, LOCAL_TMP_TXT_PATH\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv() \n",
    "# Access_key = os.getenv('Access_key')\n",
    "# Secret_access_key = os.getenv('Secret_access_key')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_id = 0\n",
    "FILE_KEY = FILE_KEY.format(instance_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Process read source.')\n",
    "# parser.add_argument('--mode', type=str, choices=['s3', 'local'],\n",
    "#                     help='Specify where to read files from and write to: \"s3\" for Amazon S3, \"local\" for local filesystem.')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "mode = 's3'\n",
    "\n",
    "def batch_strings(input_list, N):\n",
    "    return [input_list[i:i + N] for i in range(0, len(input_list), N)]\n",
    "\n",
    "# Use the argument to decide where to read the files from\n",
    "if mode == 's3':\n",
    "    s3_client = boto3.client('s3')#,\n",
    "                #   aws_access_key_id=Access_key,\n",
    "                #   aws_secret_access_key=Secret_access_key)\n",
    "    #I stored all my urls on s3, but since we want to load it from local, you should run it run it with local mode.\n",
    "    response = s3_client.get_object(Bucket= BULK_DATA_BUCKET_OR_PATH, Key=FILE_KEY)\n",
    "    content = response['Body'].read().decode('utf-8')\n",
    "    url_list = content.splitlines()\n",
    "elif mode == 'local':\n",
    "    with open(BULK_DATA_BUCKET_OR_PATH,'r') as f:\n",
    "        url_list = f.readlines()\n",
    "    \n",
    "    s3_client = None\n",
    "else:\n",
    "    raise Exception(\"choose 'local' or 's3' mode. eg: --mode local\")\n",
    "\n",
    "#This script was running on 3 different instances for more than a week. It won't be needed if you want to pull less than 5m urls probably,\n",
    "# or if time is not your priority.\n",
    "\n",
    "\n",
    "os.makedirs(LOCAL_TMP_DIR, exist_ok=True)\n",
    "# os.makedirs(LOCAL_TMP_DIR_ZIP, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I used this block just several times, when kernel died for unknown reasons and I wanted to continue from where I left off.\n",
    "batch_to_start_from = get_max_batch_N(s3_client, BUCKET_OR_PATH_TO_SAVE, mode, instance_id)\n",
    "\n",
    "url_list = url_list[batch_to_start_from:]\n",
    "\n",
    "#this is the list on which we will iterate. It is list of urls grouped in 1000s.\n",
    "#ThreadPoolExecutor will iterate on these batches and try to process in MAX_WORKERS number of parallel urls.\n",
    "#After Downloading 1000 htmls, unite them in parquet and write to s3.\n",
    "#I did that because I was restricted in space locally.\n",
    "\n",
    "url_list = [url.strip() for url in url_list]\n",
    "N = 1000 # replace with your desired batch size\n",
    "grouped_urls = batch_strings(url_list, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3/450292'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{batch_to_start_from}/{len(grouped_urls)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batchid, batch in tqdm(enumerate(grouped_urls)):\n",
    "    unique_id = f'{instance_id}_{batchid+batch_to_start_from}_n_{uuid.uuid4()}'\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # Use ThreadPoolExecutor to fetch and save HTML in parallel\n",
    "        executor.map(retrieve, batch)\n",
    "    # txts = ['txt_output/'+f for f in os.listdir('txt_output') if f.endswith('.html')]\n",
    "    # write_zip(tmp_txt_path,txts,s3_client,BUCKET_OR_PATH_TO_SAVE)\n",
    "    if os.path.exists(LOCAL_TMP_TXT_PATH):\n",
    "        write_parquet(unique_id, s3_client, BUCKET_OR_PATH_TO_SAVE,mode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alpaca",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
